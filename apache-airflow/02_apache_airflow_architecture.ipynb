{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7dDc7Sdx31G4YEFT+7J9V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thecodemancer/study-with-me/blob/main/apache-airflow/02_apache_airflow_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Airflow Architecture\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/720/0*Dj1KSM1cPmU90VtD.png\" />\n",
        "\n",
        "Most executors will generally also introduce other components to let them talk to their workers - like a task queue - but you can still think of the executor and its workers as a single logical component in Airflow overall, handling the actual task execution.\n",
        "\n",
        "Airflow itself is agnostic to what you’re running - it will happily orchestrate and run anything, either with high-level support from one of our providers, or directly as a command using the shell or Python Operators."
      ],
      "metadata": {
        "id": "oukG4Lpl7_yV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Workloads\n",
        "\n",
        "A DAG runs through a series of Tasks, and there are three common types of task you will see:\n",
        "\n",
        "- Operators, predefined tasks that you can string together quickly to build most parts of your DAGs.\n",
        "\n",
        "- Sensors, a special subclass of Operators which are entirely about waiting for an external event to happen.\n",
        "\n",
        "- A TaskFlow-decorated @task, which is a custom Python function packaged up as a Task.\n",
        "\n",
        "Internally, these are all actually subclasses of Airflow’s BaseOperator, and the concepts of Task and Operator are somewhat interchangeable, but it’s useful to think of them as separate concepts - essentially, Operators and Sensors are templates, and when you call one in a DAG file, you’re making a Task."
      ],
      "metadata": {
        "id": "lXkAE3zCJiiv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Control Flow\n",
        "\n",
        "DAGs are designed to be run many times, and multiple runs of them can happen in parallel. DAGs are parameterized, always including an interval they are “running for” (the data interval), but with other optional parameters as well.\n",
        "\n",
        "Tasks have dependencies declared on each other. You’ll see this in a DAG either using the >> and << operators:\n",
        "\n",
        "```\n",
        "first_task >> [second_task, third_task]\n",
        "fourth_task << third_task\n",
        "```\n",
        "\n",
        "Or, with the set_upstream and set_downstream methods:\n",
        "\n",
        "```\n",
        "first_task.set_downstream([second_task, third_task])\n",
        "fourth_task.set_upstream(third_task)\n",
        "```\n",
        "\n",
        "These dependencies are what make up the “edges” of the graph, and how Airflow works out which order to run your tasks in. By default, a task will wait for all of its upstream tasks to succeed before it runs, but this can be customized using features like Branching, LatestOnly, and Trigger Rules."
      ],
      "metadata": {
        "id": "3Q74VBlFJl3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To pass data between tasks you have three options:\n",
        "\n",
        "- XComs (“Cross-communications”), a system where you can have tasks push and pull small bits of metadata.\n",
        "- Uploading and downloading large files from a storage service (either one you run, or part of a public cloud)\n",
        "- TaskFlow API automatically passes data between tasks via implicit XComs\n",
        "\n",
        "Airflow sends out Tasks to run on Workers as space becomes available, so there’s no guarantee all the tasks in your DAG will run on the same worker or the same machine.\n",
        "\n",
        "As you build out your DAGs, they are likely to get very complex, so Airflow provides several mechanisms for making this more sustainable - SubDAGs let you make “reusable” DAGs you can embed into other ones, and TaskGroups let you visually group tasks in the UI.\n",
        "\n",
        "There are also features for letting you easily pre-configure access to a central resource, like a datastore, in the form of Connections & Hooks, and for limiting concurrency, via Pools."
      ],
      "metadata": {
        "id": "fMppS4RxJ2UP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User interface\n",
        "\n",
        "Airflow comes with a user interface that lets you see what DAGs and their tasks are doing, trigger runs of DAGs, view logs, and do some limited debugging and resolution of problems with your DAGs.\n",
        "\n",
        "<img src=\"https://airflow.apache.org/docs/apache-airflow/stable/_images/dags.png\" />\n",
        "\n",
        "It’s generally the best way to see the status of your Airflow installation as a whole, as well as diving into individual DAGs to see their layout, the status of each task, and the logs from each task."
      ],
      "metadata": {
        "id": "--zf55JVJwY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "If you made it this far, follow [David Regalado](https://beacons.ai/davidregalado) for more code!"
      ],
      "metadata": {
        "id": "L7XbzPJmLRpP"
      }
    }
  ]
}