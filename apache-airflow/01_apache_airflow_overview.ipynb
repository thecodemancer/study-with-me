{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMT8SxeBYzInUWSpYRJUkon",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thecodemancer/study-with-me/blob/main/apache-airflow/01_apache_airflow_overview.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<ceenter><img src=\"https://i0.wp.com/blog.knoldus.com/wp-content/uploads/2021/05/pasted-image-0-1.png?fit=549%2C215&ssl=1\" /></center>\n",
        "\n",
        "# What is Apache Airflow?\n",
        "\n",
        "Apache Airflow (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows.\n",
        "\n",
        "When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative.\n",
        "\n",
        "A workflow is represented as a DAG (a Directed Acyclic Graph), and contains individual pieces of work called Tasks, arranged with dependencies and data flows taken into account.\n",
        "\n",
        "A DAG specifies the dependencies between Tasks, and the order in which to execute them and run retries; the Tasks themselves describe what to do, be it fetching data, running analysis, triggering other systems, or more."
      ],
      "metadata": {
        "id": "ym-NZ1J-yHAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An Airflow installation generally consists of the following components:\n",
        "\n",
        "- A **scheduler**, which handles both triggering scheduled workflows, and submitting Tasks to the executor to run.\n",
        "- An **executor**, which handles running tasks. In the default Airflow installation, this runs everything inside the scheduler, but most production-suitable executors actually push task execution out to workers.\n",
        "- A **webserver**, which presents a handy user interface to inspect, trigger and debug the behaviour of DAGs and tasks.\n",
        "- A folder of **DAG** files, read by the scheduler and executor (and any workers the executor has)\n",
        "- A **metadata** database, used by the scheduler, executor and webserver to store state."
      ],
      "metadata": {
        "id": "5hs5b7CKL5ww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Focus\n",
        "\n",
        "Airflow works best with workflows that are mostly static and slowly changing. When the DAG structure is similar from one run to the next, it clarifies the unit of work and continuity. Other similar projects include ***Luigi***, ***Oozie*** and ***Azkaban***.\n",
        "\n",
        "Airflow is commonly used to process data, but has the opinion that tasks should ideally be idempotent (i.e., results of the task will be the same, and will not create duplicated data in a destination system), and should not pass large quantities of data from one task to the next (though tasks can pass metadata using Airflow's Xcom feature). For high-volume, data-intensive tasks, a best practice is to delegate to external services specializing in that type of work.\n",
        "\n",
        "Airflow is not a streaming solution, but it is often used to process real-time data, pulling data off streams in batches."
      ],
      "metadata": {
        "id": "80qpvnseQmDV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principles\n",
        "\n",
        "- **Dynamic**: Airflow pipelines are configuration as code (Python), allowing for dynamic pipeline generation. This allows for writing code that instantiates pipelines dynamically.\n",
        "- **Extensible**: Easily define your own operators, executors and extend the library so that it fits the level of abstraction that suits your environment.\n",
        "- **Elegant**: Airflow pipelines are lean and explicit. Parameterizing your scripts is built into the core of Airflow using the powerful Jinja templating engine.\n",
        "- **Scalable**: Airflow has a modular architecture and uses a message queue to orchestrate an arbitrary number of workers."
      ],
      "metadata": {
        "id": "Inf6GFtqQqk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "If you made it this far, follow [David Regalado](https://beacons.ai/davidregalado) for more code!"
      ],
      "metadata": {
        "id": "L7XbzPJmLRpP"
      }
    }
  ]
}